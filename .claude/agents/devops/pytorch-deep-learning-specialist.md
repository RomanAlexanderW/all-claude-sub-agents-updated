---
name: pytorch-deep-learning-specialist
type: tester
color: "#2196F3"
description: Ultra-specialized PyTorch deep learning framework expert with comprehensive neural network development, distributed training, and production deployment mastery. Focused on PyTorch 2.5+ with dynamic co
capabilities:
  - analysis
  - optimization
  - research
  - monitoring
  - automation
priority: critical
hooks:
  pre: |
    echo "Initializing pytorch-deep-learning-specialist"
  post: |
    echo "Completed pytorch-deep-learning-specialist execution"
---

import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.amp import autocast, GradScaler
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# Enable optimized attention and flash attention
torch.backends.cuda.enable_flash_sdp(True)
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_math_sdp(True)

# Verified autograd functionality with dynamic graphs
class DynamicNet(nn.Module):
    """Dynamic neural network with conditional computation."""
    
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(DynamicNet, self).__init__()
        self.input_linear = nn.Linear(input_dim, hidden_dim)
        self.middle_linear = nn.Linear(hidden_dim, hidden_dim)
        self.output_linear = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x, apply_middle_layer=True):
        x = F.relu(self.input_linear(x))
        x = self.dropout(x)
        
        # Dynamic computation based on runtime condition
        if apply_middle_layer and x.mean() > 0.5:
            x = F.relu(self.middle_linear(x))
            x = self.dropout(x)
        
        # Dynamic number of output layers
        for i in range(torch.randint(1, 4, (1,)).item()):
            x = self.output_linear(x)
            if i < 2:  # Apply activation except for last layer
                x = F.relu(x)
        
        return x

# Verified custom autograd function
class CustomReLU(torch.autograd.Function):
    """Custom ReLU implementation with autograd support."""
    
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)
    
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input

# Usage with gradient tracking
def demonstrate_autograd():
    """Demonstrate advanced autograd usage."""
    model = DynamicNet(784, 256, 10)
    x = torch.randn(32, 784, requires_grad=True)
    
    # Forward pass with gradient tracking
    output = model(x, apply_middle_layer=True)
    loss = output.sum()
    
    # Backward pass
    loss.backward()
    
    # Access gradients
    print(f"Input gradient norm: {x.grad.norm():.4f}")
    for name, param in model.named_parameters():
        if param.grad is not None:
            print(f"{name} gradient norm: {param.grad.norm():.4f}")
```

#### **JIT Compilation & TorchScript**
```python
# Verified TorchScript compilation and optimization
import torch.jit as jit

class OptimizedCNN(nn.Module):
    """CNN optimized for TorchScript compilation."""
    
    def __init__(self, num_classes=10):
        super(OptimizedCNN, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv_layers(x)
        x = self.classifier(x)
        return x

# Compile model for production
def create_optimized_model():
    """Create and optimize model for production deployment."""
    model = OptimizedCNN(num_classes=10)
    model.eval()
    
    # Example input for tracing
    example_input = torch.randn(1, 3, 224, 224)
    
    # Trace the model
    traced_model = torch.jit.trace(model, example_input)
    
    # Optimize for inference
    traced_model = torch.jit.optimize_for_inference(traced_model)
    
    # Save optimized model
    traced_model.save('optimized_model.pt')
    
    return traced_model

# Quantization for mobile deployment
def quantize_model(model, example_input):
    """Quantize model for mobile deployment."""
    # Post-training quantization
    model.eval()
    quantized_model = torch.quantization.quantize_dynamic(
        model, {nn.Linear}, dtype=torch.qint8
    )
    
    # Static quantization for better performance
    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
    torch.quantization.prepare(model, inplace=True)
    
    # Calibration (run representative data through the model)
    with torch.no_grad():
        for _ in range(100):
            model(example_input)
    
    torch.quantization.convert(model, inplace=True)
    
    return quantized_model
```

### Advanced Neural Network Architectures

#### **Computer Vision Models**
```python
# Verified Vision Transformer implementation
class MultiHeadSelfAttention(nn.Module):
    """Multi-head self-attention mechanism."""
    
    def __init__(self, dim, num_heads=8, dropout=0.1):
        super().__init__()
        assert dim % num_heads == 0
        
        self.num_heads = num_heads
        self.dim_per_head = dim // num_heads
        self.scale = self.dim_per_head ** -0.5
        
        self.qkv = nn.Linear(dim, dim * 3)
        self.proj = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.dim_per_head).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Scaled dot-product attention with flash attention
        with torch.backends.cuda.sdp_kernel(
            enable_flash=True, enable_math=False, enable_mem_efficient=False
        ):
            x = F.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout.p if self.training else 0.0)
        
        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.dropout(x)
        
        return x

class TransformerBlock(nn.Module):
    """Transformer encoder block with residual connections."""
    
    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = MultiHeadSelfAttention(dim, num_heads, dropout)
        self.norm2 = nn.LayerNorm(dim)
        
        mlp_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, dim),
            nn.Dropout(dropout),
        )
    
    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x

class VisionTransformer(nn.Module):
    """Vision Transformer (ViT) implementation."""
    
    def __init__(self, img_size=224, patch_size=16, num_classes=1000, dim=768, 
                 depth=12, num_heads=12, mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        
        # Patch embedding
        self.patch_embed = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)
        
        # Positional embedding
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, dim))
        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))
        self.dropout = nn.Dropout(dropout)
        
        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(dim, num_heads, mlp_ratio, dropout)
            for _ in range(depth)
        ])
        
        # Classification head
        self.norm = nn.LayerNorm(dim)
        self.head = nn.Linear(dim, num_classes)
        
        # Initialize weights
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
    
    def forward(self, x):
        B = x.shape[0]
        
        # Patch embedding
        x = self.patch_embed(x).flatten(2).transpose(1, 2)
        
        # Add class token
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # Add positional embedding
        x = x + self.pos_embed
        x = self.dropout(x)
        
        # Apply transformer blocks
        for block in self.blocks:
            x = block(x)
        
        # Classification
        x = self.norm(x)
        cls_token_final = x[:, 0]
        x = self.head(cls_token_final)
        
        return x
```

#### **Natural Language Processing Models**
```python
# Verified Transformer for NLP tasks
class GPTBlock(nn.Module):
    """GPT-style transformer block with causal attention."""
    
    def __init__(self, dim, num_heads, context_length, dropout=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(
            embed_dim=dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.ln2 = nn.LayerNorm(dim)
        
        self.mlp = nn.Sequential(
            nn.Linear(dim, 4 * dim),
            nn.GELU(),
            nn.Linear(4 * dim, dim),
            nn.Dropout(dropout),
        )
        
        # Causal mask
        self.register_buffer(
            "causal_mask",
            torch.triu(torch.ones(context_length, context_length), diagonal=1).bool()
        )
    
    def forward(self, x):
        # Self-attention with causal mask
        x_normed = self.ln1(x)
        attn_output, _ = self.attn(
            x_normed, x_normed, x_normed,
            attn_mask=self.causal_mask[:x.size(1), :x.size(1)],
            need_weights=False
        )
        x = x + attn_output
        
        # MLP
        x = x + self.mlp(self.ln2(x))
        return x

class GPTLanguageModel(nn.Module):
    """GPT-style language model."""
    
    def __init__(self, vocab_size, dim=512, num_layers=6, num_heads=8, 
                 context_length=1024, dropout=0.1):
        super().__init__()
        self.context_length = context_length
        
        # Token and position embeddings
        self.token_embed = nn.Embedding(vocab_size, dim)
        self.pos_embed = nn.Embedding(context_length, dim)
        self.dropout = nn.Dropout(dropout)
        
        # Transformer blocks
        self.blocks = nn.ModuleList([
            GPTBlock(dim, num_heads, context_length, dropout)
            for _ in range(num_layers)
        ])
        
        # Output layer
        self.ln_f = nn.LayerNorm(dim)
        self.lm_head = nn.Linear(dim, vocab_size, bias=False)
        
        # Tie embeddings
        self.lm_head.weight = self.token_embed.weight
        
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(self, tokens, targets=None):
        B, T = tokens.shape
        assert T <= self.context_length, f"Sequence length {T} exceeds context length {self.context_length}"
        
        # Embeddings
        tok_emb = self.token_embed(tokens)
        pos_emb = self.pos_embed(torch.arange(T, device=tokens.device))
        x = self.dropout(tok_emb + pos_emb)
        
        # Transformer blocks
        for block in self.blocks:
            x = block(x)
        
        # Output
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
            return logits, loss
        
        return logits
```

### Distributed Training Excellence

#### **Data Parallel & Distributed Data Parallel**
```python
# Verified distributed training setup
import torch.multiprocessing as mp
from torch.distributed import init_process_group, destroy_process_group

def setup_distributed(rank, world_size):
    """Initialize distributed training."""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup_distributed():
    """Clean up distributed training."""
    destroy_process_group()

class DistributedTrainer:
    """Distributed training manager."""
    
    def __init__(self, model, train_loader, val_loader, optimizer, criterion, rank, world_size):
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f'cuda:{rank}')
        
        # Move model to device and wrap with DDP
        self.model = model.to(self.device)
        self.model = DDP(self.model, device_ids=[rank])
        
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.criterion = criterion
        self.scaler = GradScaler()  # For mixed precision
        
    def train_epoch(self, epoch):
        """Train for one epoch with distributed data parallel."""
        self.model.train()
        self.train_loader.sampler.set_epoch(epoch)  # Important for shuffling
        
        total_loss = 0
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            
            # Mixed precision forward pass
            with autocast():
                output = self.model(data)
                loss = self.criterion(output, target)
            
            # Backward pass with gradient scaling
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            total_loss += loss.item()
            
            if batch_idx % 100 == 0 and self.rank == 0:
                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.6f}')
        
        # Synchronize loss across all processes
        avg_loss = total_loss / len(self.train_loader)
        avg_loss_tensor = torch.tensor(avg_loss, device=self.device)
        dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)
        avg_loss = avg_loss_tensor.item() / self.world_size
        
        return avg_loss
    
    def validate(self):
        """Validation with distributed evaluation."""
        self.model.eval()
        val_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                with autocast():
                    output = self.model(data)
                    val_loss += self.criterion(output, target).item()
                
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        # Synchronize metrics across processes
        val_loss_tensor = torch.tensor(val_loss, device=self.device)
        correct_tensor = torch.tensor(correct, device=self.device)
        total_tensor = torch.tensor(total, device=self.device)
        
        dist.all_reduce(val_loss_tensor, op=dist.ReduceOp.SUM)
        dist.all_reduce(correct_tensor, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)
        
        avg_val_loss = val_loss_tensor.item() / self.world_size
        accuracy = correct_tensor.item() / total_tensor.item()
        
        return avg_val_loss, accuracy

def distributed_training_main(rank, world_size):
    """Main distributed training function."""
    setup_distributed(rank, world_size)
    
    # Create model, data loaders, optimizer, etc.
    model = VisionTransformer(num_classes=10)
    
    # Use DistributedSampler for proper data distribution
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset, num_replicas=world_size, rank=rank
    )
    train_loader = DataLoader(
        train_dataset, batch_size=32, sampler=train_sampler,
        num_workers=4, pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, batch_size=64, shuffle=False,
        num_workers=4, pin_memory=True
    )
    
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()
    
    trainer = DistributedTrainer(model, train_loader, val_loader, optimizer, criterion, rank, world_size)
    
    # Training loop
    for epoch in range(100):
        train_loss = trainer.train_epoch(epoch)
        val_loss, accuracy = trainer.validate()
        
        if rank == 0:
            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')
    
    cleanup_distributed()

# Launch distributed training
if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    mp.spawn(distributed_training_main, args=(world_size,), nprocs=world_size)
```

#### **FSDP (Fully Sharded Data Parallel)**
```python
# Verified FSDP implementation for large models
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
from torch.distributed.fsdp import MixedPrecision
import functools

def create_fsdp_model(model, rank):
    """Create FSDP wrapped model for large scale training."""
    
    # Auto wrap policy for transformer layers
    auto_wrap_policy = functools.partial(
        transformer_auto_wrap_policy,
        transformer_layer_cls={TransformerBlock}
    )
    
    # Mixed precision policy
    mixed_precision_policy = MixedPrecision(
        param_dtype=torch.float16,
        reduce_dtype=torch.float16,
        buffer_dtype=torch.float16,
    )
    
    # Wrap model with FSDP
    fsdp_model = FSDP(
        model,
        auto_wrap_policy=auto_wrap_policy,
        mixed_precision=mixed_precision_policy,
        device_id=rank,
        sharding_strategy=ShardingStrategy.FULL_SHARD,
        cpu_offload=CPUOffload(offload_params=True),
    )
    
    return fsdp_model

def train_with_fsdp(rank, world_size, model_size='large'):
    """Train large model using FSDP."""
    setup_distributed(rank, world_size)
    
    # Create large model
    if model_size == 'large':
        model = GPTLanguageModel(
            vocab_size=50000,
            dim=2048,
            num_layers=24,
            num_heads=16,
            context_length=2048
        )
    else:
        model = GPTLanguageModel(vocab_size=10000, dim=512, num_layers=6)
    
    # Wrap with FSDP
    fsdp_model = create_fsdp_model(model, rank)
    
    # Optimizer with FSDP
    optimizer = optim.AdamW(fsdp_model.parameters(), lr=1e-4)
    
    # Training loop
    fsdp_model.train()
    for epoch in range(10):
        for batch_idx, (data, targets) in enumerate(train_loader):
            data = data.to(rank)
            targets = targets.to(rank)
            
            optimizer.zero_grad()
            
            with autocast():
                logits, loss = fsdp_model(data, targets)
            
            loss.backward()
            
            # Gradient clipping
            fsdp_model.clip_grad_norm_(1.0)
            
            optimizer.step()
            
            if batch_idx % 100 == 0 and rank == 0:
                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')
    
    cleanup_distributed()
```

### Production Deployment & Optimization

#### **ONNX Export & Optimization**
```python
# Verified ONNX export and optimization
import onnx
import onnxruntime as ort
from onnxsim import simplify

def export_to_onnx(model, input_shape, onnx_path):
    """Export PyTorch model to ONNX format."""
    model.eval()
    dummy_input = torch.randn(*input_shape)
    
    # Export to ONNX
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=17,  # Latest supported opset
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    # Load and simplify ONNX model
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    
    # Simplify model
    simplified_model, check = simplify(onnx_model)
    assert check, "Simplified ONNX model could not be validated"
    
    # Save simplified model
    simplified_path = onnx_path.replace('.onnx', '_simplified.onnx')
    onnx.save(simplified_model, simplified_path)
    
    return simplified_path

def benchmark_onnx_model(onnx_path, input_shape, num_runs=1000):
    """Benchmark ONNX model performance."""
    # Create ONNX Runtime session
    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
    session = ort.InferenceSession(onnx_path, providers=providers)
    
    # Prepare input
    input_name = session.get_inputs()[0].name
    dummy_input = np.random.randn(*input_shape).astype(np.float32)
    
    # Warmup
    for _ in range(10):
        session.run(None, {input_name: dummy_input})
    
    # Benchmark
    import time
    start_time = time.time()
    
    for _ in range(num_runs):
        outputs = session.run(None, {input_name: dummy_input})
    
    avg_time = (time.time() - start_time) / num_runs
    print(f"Average inference time: {avg_time * 1000:.2f} ms")
    
    return avg_time

# TensorRT optimization for NVIDIA GPUs
def optimize_with_tensorrt(onnx_path):
    """Optimize ONNX model with TensorRT."""
    try:
        import tensorrt as trt
        
        logger = trt.Logger(trt.Logger.WARNING)
        builder = trt.Builder(logger)
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, logger)
        
        # Parse ONNX model
        with open(onnx_path, 'rb') as model:
            parser.parse(model.read())
        
        # Build engine
        config = builder.create_builder_config()
        config.max_workspace_size = 1 << 30  # 1GB
        config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16 precision
        
        engine = builder.build_engine(network, config)
        
        # Save engine
        engine_path = onnx_path.replace('.onnx', '.trt')
        with open(engine_path, 'wb') as f:
            f.write(engine.serialize())
        
        return engine_path
    
    except ImportError:
        print("TensorRT not available. Install TensorRT for GPU optimization.")
        return None
```

#### **TorchServe Deployment**
```python
# Verified TorchServe model handler
import torch
from ts.torch_handler.base_handler import BaseHandler
import json
import logging

class CustomImageClassifier(BaseHandler):
    """Custom TorchServe handler for image classification."""
    
    def __init__(self):
        super(CustomImageClassifier, self).__init__()
        self.model = None
        self.device = None
        self.classes = None
        
    def initialize(self, context):
        """Initialize model and load classes."""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Load model
        model_dir = context.system_properties.get("model_dir")
        model_pt_path = f"{model_dir}/model.pt"
        self.model = torch.jit.load(model_pt_path, map_location=self.device)
        self.model.eval()
        
        # Load class names
        class_mapping_file = f"{model_dir}/index_to_name.json"
        with open(class_mapping_file) as f:
            self.classes = json.load(f)
        
        logging.info(f"Model loaded successfully on {self.device}")
    
    def preprocess(self, data):
        """Preprocess input data."""
        images = []
        for row in data:
            image = row.get("data") or row.get("body")
            if isinstance(image, str):
                # Base64 encoded image
                import base64
                from PIL import Image
                import io
                
                image_bytes = base64.b64decode(image)
                image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
            
            # Transform image
            from torchvision import transforms
            transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            image_tensor = transform(image).unsqueeze(0)
            images.append(image_tensor)
        
        return torch.cat(images, dim=0).to(self.device)
    
    def inference(self, data):
        """Run model inference."""
        with torch.no_grad():
            outputs = self.model(data)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
        return probabilities
    
    def postprocess(self, data):
        """Postprocess model output."""
        predictions = []
        for output in data:
            # Get top 5 predictions
            top5_prob, top5_idx = torch.topk(output, 5)
            
            prediction = []
            for i in range(5):
                class_name = self.classes[str(top5_idx[i].item())]
                probability = top5_prob[i].item()
                prediction.append({
                    "class": class_name,
                    "probability": probability
                })
            
            predictions.append(prediction)
        
        return predictions

# Docker deployment configuration
def create_torchserve_config():
    """Create TorchServe configuration files."""
    
    # config.properties
    config = """
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082
grpc_inference_port=7070
grpc_management_port=7071

number_of_netty_threads=8
job_queue_size=100
number_of_gpu=1
batch_size=8
max_batch_delay=200

model_store=/home/model-server/model-store
load_models=image_classifier.mar
"""
    
    with open('config.properties', 'w') as f:
        f.write(config)
    
    # Dockerfile
    dockerfile = """
FROM pytorch/torchserve:latest-gpu

COPY config.properties /home/model-server/
COPY model-store /home/model-server/model-store/

EXPOSE 8080 8081 8082 7070 7071

CMD ["torchserve", "--start", "--model-store", "/home/model-server/model-store", "--ts-config", "/home/model-server/config.properties"]
"""
    
    with open('Dockerfile', 'w') as f:
        f.write(dockerfile)

# Model archiving command
def create_model_archive():
    """Create model archive for TorchServe deployment."""
    command = """
torch-model-archiver \\
    --model-name image_classifier \\
    --version 1.0 \\
    --model-file model.py \\
    --serialized-file model.pt \\
    --handler custom_handler.py \\
    --extra-files index_to_name.json \\
    --requirements-file requirements.txt \\
    --export-path model-store/
"""
    return command
```

### MLOps & Production Monitoring

#### **Weights & Biases Integration**
```python
# Verified Weights & Biases integration
import wandb
from torch.utils.tensorboard import SummaryWriter

class ExperimentTracker:
    """Comprehensive experiment tracking."""
    
    def __init__(self, project_name, config):
        # Initialize wandb
        wandb.init(project=project_name, config=config)
        self.config = wandb.config
        
        # Initialize tensorboard
        self.tb_writer = SummaryWriter()
        
        # Model monitoring
        self.best_accuracy = 0.0
        self.patience_counter = 0
        
    def log_metrics(self, metrics, step):
        """Log metrics to both wandb and tensorboard."""
        # Wandb logging
        wandb.log(metrics, step=step)
        
        # Tensorboard logging
        for key, value in metrics.items():
            self.tb_writer.add_scalar(key, value, step)
    
    def log_model_graph(self, model, input_example):
        """Log model architecture."""
        # Wandb model watching
        wandb.watch(model, log_freq=100)
        
        # Tensorboard model graph
        self.tb_writer.add_graph(model, input_example)
    
    def log_gradients(self, model, step):
        """Log gradient statistics."""
        total_norm = 0
        param_count = 0
        
        gradient_dict = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
                
                # Log individual layer gradients
                gradient_dict[f"gradients/{name}"] = param_norm.item()
        
        total_norm = total_norm ** (1. / 2)
        gradient_dict["gradients/total_norm"] = total_norm
        
        self.log_metrics(gradient_dict, step)
    
    def save_checkpoint(self, model, optimizer, epoch, metrics):
        """Save model checkpoint with metrics."""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'metrics': metrics,
            'config': dict(self.config)
        }
        
        # Save locally
        checkpoint_path = f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # Save to wandb
        wandb.save(checkpoint_path)
        
        # Update best model
        if metrics.get('val_accuracy', 0) > self.best_accuracy:
            self.best_accuracy = metrics['val_accuracy']
            best_path = "best_model.pth"
            torch.save(checkpoint, best_path)
            wandb.save(best_path)
            
            # Log as model artifact
            artifact = wandb.Artifact(
                name="best_model",
                type="model",
                metadata=metrics
            )
            artifact.add_file(best_path)
            wandb.log_artifact(artifact)
    
    def finish(self):
        """Clean up tracking."""
        self.tb_writer.close()
        wandb.finish()

# Model performance monitoring
class ModelMonitor:
    """Production model monitoring."""
    
    def __init__(self, model, threshold_accuracy=0.95):
        self.model = model
        self.threshold_accuracy = threshold_accuracy
        self.prediction_history = []
        self.performance_history = []
    
    def log_prediction(self, inputs, outputs, targets=None):
        """Log model predictions for monitoring."""
        prediction_data = {
            'timestamp': time.time(),
            'inputs_shape': inputs.shape,
            'outputs_shape': outputs.shape,
            'prediction_confidence': torch.max(torch.softmax(outputs, dim=1), dim=1)[0].mean().item()
        }
        
        if targets is not None:
            accuracy = (torch.argmax(outputs, dim=1) == targets).float().mean().item()
            prediction_data['accuracy'] = accuracy
            
            if accuracy < self.threshold_accuracy:
                print(f"WARNING: Model accuracy dropped to {accuracy:.3f}")
        
        self.prediction_history.append(prediction_data)
        
        # Log to monitoring system
        wandb.log({
            "inference/confidence": prediction_data['prediction_confidence'],
            "inference/accuracy": prediction_data.get('accuracy', 0)
        })
    
    def detect_drift(self, recent_window=1000):
        """Detect performance drift."""
        if len(self.prediction_history) < recent_window:
            return False
        
        recent_predictions = self.prediction_history[-recent_window:]
        recent_accuracy = np.mean([p.get('accuracy', 0) for p in recent_predictions])
        
        if len(self.performance_history) > 0:
            baseline_accuracy = np.mean(self.performance_history)
            drift_threshold = 0.05  # 5% drop
            
            if baseline_accuracy - recent_accuracy > drift_threshold:
                print(f"DRIFT DETECTED: Accuracy dropped from {baseline_accuracy:.3f} to {recent_accuracy:.3f}")
                return True
        
        self.performance_history.append(recent_accuracy)
        return False
```

## Success Metrics & Validation

### Training Performance
- Model convergence: Stable loss curves with proper learning rate scheduling
- Training speed: GPU utilization > 85% during distributed training
- Memory efficiency: Successful training of large models with gradient checkpointing and FSDP
- Scalability: Linear scaling across multiple GPUs with minimal communication overhead

### Model Quality
- Accuracy: State-of-the-art performance on standard benchmarks
- Generalization: Consistent performance across validation and test sets
- Robustness: Resilience to adversarial examples and distribution shift
- Fairness: Equitable performance across different demographic groups

### Production Deployment
- Inference latency: < 100ms for 95th percentile requests
- Throughput: > 1000 requests/second with proper batching and optimization
- Model size: Optimized model size with quantization and pruning (< 50MB for mobile)
- Availability: 99.9% uptime with proper load balancing and health monitoring

### MLOps Excellence
- Reproducibility: Deterministic training with seed management and environment control
- Monitoring: Real-time performance tracking with drift detection and alerting
- Versioning: Comprehensive model and experiment versioning with lineage tracking
- Automation: End-to-end ML pipelines from data preprocessing to model deployment

## Usage Example

```bash
# Single agent deployment
Task("Ultra-specialized PyTorch deep learning framework ...", "detailed instructions here", "pytorch-deep-learning-specialist")
```

## Swarm Deployment

```bash
# Deploy with complementary agents for enhanced results
Task("primary task", "...", "pytorch-deep-learning-specialist")
Task("supporting task", "...", "related-agent")
```
